{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aggel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\aggel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download necessary files\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Reuters Corpus + Building Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8b45b3c-675b-4639-842f-378d3b7ae2b9",
    "outputId": "65e8d8d9-a444-4806-d2a7-a6b73e672741"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replace OOV with '*UNK*' in train_corpus: 100%|██████████████████████████████████| 3000/3000 [00:00<00:00, 4696.00it/s]\n",
      "Replace OOV with '*UNK*' in test_corpus: 100%|█████████████████████████████████████| 600/600 [00:00<00:00, 4577.56it/s]\n",
      "Replace OOV with '*UNK*' in develpment_corpus: 100%|███████████████████████████████| 300/300 [00:00<00:00, 4503.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187\n",
      "18961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#query (i)a Corpus and vocabualry preprocesing\n",
    "#------------------\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load train and test corpus\n",
    "corpus = nltk.corpus.reuters.sents()[:3000]  # Sample train_corpus\n",
    "\n",
    "test_corpus = nltk.corpus.reuters.sents()[18000:18600]  # Sample test_corpus\n",
    "\n",
    "development_corpus = nltk.corpus.reuters.sents()[19000:19300]  # Sample test_corpus\n",
    "\n",
    "def build_vocabulary(corpus, min_frequency):\n",
    "    word_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            word_counts[word] += 1\n",
    "    vocabulary = {word for word, count in word_counts.items() if count >= min_frequency}\n",
    "    vocabulary.add('*UNK*')\n",
    "    vocabulary.add('*end*')\n",
    "    return vocabulary\n",
    "\n",
    "# # 2. Minimum Count Threshold for OOV words\n",
    "min_frequency = 10\n",
    "\n",
    "vocabulary = build_vocabulary(corpus, min_frequency)\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "modified_corpus = [list(sentence) for sentence in corpus]\n",
    "\n",
    "modified_test_corpus = [list(sentence) for sentence in test_corpus]\n",
    "\n",
    "modified_development_corpus = [list(sentence) for sentence in development_corpus]\n",
    "\n",
    "# Initialize the counter for *UNK* token\n",
    "unk_count = 0\n",
    "\n",
    "# Replacing OOV words with the token *UNK* in train corpus\n",
    "for sentence in tqdm(range(len(modified_corpus)), desc=\"Replace OOV with '*UNK*' in train_corpus\"):\n",
    "    for word_idx in range(len(modified_corpus[sentence])):\n",
    "        word = modified_corpus[sentence][word_idx]\n",
    "        if word not in vocabulary:\n",
    "            modified_corpus[sentence][word_idx] = '*UNK*'\n",
    "            unk_count += 1\n",
    "\n",
    "corpus = modified_corpus\n",
    "\n",
    "# Replacing OOV words with the token *UNK* in test_corpus\n",
    "for sentence in tqdm(range(len(modified_test_corpus)), desc=\"Replace OOV with '*UNK*' in test_corpus\"):\n",
    "    for word_idx in range(len(modified_test_corpus[sentence])):\n",
    "        word = modified_test_corpus[sentence][word_idx]\n",
    "        if word not in vocabulary:\n",
    "            modified_test_corpus[sentence][word_idx] = '*UNK*'\n",
    "\n",
    "test_corpus = modified_test_corpus\n",
    "\n",
    "\n",
    "# Replacing OOV words with the token *UNK* in development_corpus\n",
    "for sentence in tqdm(range(len(modified_development_corpus)), desc=\"Replace OOV with '*UNK*' in develpment_corpus\"):\n",
    "    for word_idx in range(len(modified_development_corpus[sentence])):\n",
    "        word = modified_development_corpus[sentence][word_idx]\n",
    "        if word not in vocabulary:\n",
    "            modified_development_corpus[sentence][word_idx] = '*UNK*'\n",
    "\n",
    "development_corpus = modified_development_corpus\n",
    "\n",
    "\n",
    "print(len(vocabulary))\n",
    "print(unk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Bigram & Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d173c648-784a-4afa-a028-8469461b80d3",
    "outputId": "0c27efbe-144c-4a65-ba6e-b38e68689cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Bigram model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2095.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Trigram model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1267.99it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:18<00:00,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#query (i)b Creating the models\n",
    "#------------------\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to build a bigram model with Kneser-Ney smoothing\n",
    "def build_bigram_model_kneser_ney(corpus, vocabulary, discount):\n",
    "    # Create a copy of the vocabulary and add '*start*' to it\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.append('*start*')\n",
    "    \n",
    "    # Create a 2D array to represent the bigram model\n",
    "    bigram_model = [[0. for _ in temp_vocabulary] for _ in temp_vocabulary]\n",
    "    \n",
    "    # Initialize count dictionaries for Kneser-Ney smoothing\n",
    "    unigram_count = [0.0] * len(temp_vocabulary)\n",
    "    continuation_count = [0.0] * len(temp_vocabulary)\n",
    "    \n",
    "    # Process each sentence in the corpus using tqdm for progress tracking\n",
    "    for sentence in tqdm(corpus, desc=\"Processing sentences\"):\n",
    "        # Add '*start*' and '*end*' tokens to the sentence\n",
    "        sentence = ['*start*'] + sentence + ['*end*']\n",
    "        for i in range(len(sentence) - 1):\n",
    "            word1 = sentence[i]\n",
    "            word2 = sentence[i+1]\n",
    "            \n",
    "            # Get the indices of the two words in the vocabulary\n",
    "            word1_idx = temp_vocabulary.index(word1)\n",
    "            word2_idx = temp_vocabulary.index(word2)\n",
    "\n",
    "            # Update the bigram model and count dictionaries\n",
    "            bigram_model[word1_idx][word2_idx] += 1.0\n",
    "            unigram_count[word1_idx] += 1.0\n",
    "            continuation_count[word2_idx] += 1.0\n",
    "\n",
    "    # smaller discount moves less probability mass to lower grams making the smoothing proccess weeker         \n",
    "    # Apply Kneser-Ney smoothing to the bigram model\n",
    "    for word1 in range(len(temp_vocabulary)):\n",
    "        for word2 in range(len(temp_vocabulary)):\n",
    "\n",
    "            p_continuation = continuation_count[word2] / sum(continuation_count)\n",
    "            \n",
    "            # Handle division by zero and apply smoothing\n",
    "            lambda_value = (discount / (unigram_count[word1] + unk_count)) * len(continuation_count)\n",
    "            \n",
    "            p_continuation = max(p_continuation - discount, 0)\n",
    "            \n",
    "            # Smooth the bigram model\n",
    "            prob = (max(bigram_model[word1][word2] - discount, 0) / (unigram_count[word1] + unk_count)) + lambda_value * p_continuation\n",
    "\n",
    "            bigram_model[word1][word2] = prob\n",
    "\n",
    "    # Return the smoothed bigram model\n",
    "    return bigram_model\n",
    "\n",
    "# smaller discount moves less probability mass to lower grams making the smoothing proccess weeker \n",
    "discount = 0.0005\n",
    "\n",
    "# Print a message to indicate that we are building the bigram model\n",
    "print(\"Building Bigram model ...\")\n",
    "\n",
    "# Call the function to build the bigram model and store the result in bigram_model\n",
    "bigram_model = build_bigram_model_kneser_ney(corpus, vocabulary, discount)\n",
    "\n",
    "# Define a function to build a trigram model with Kneser-Ney smoothing\n",
    "def build_trigram_model_kneser_ney(corpus, vocabulary, discount):\n",
    "    # Create a copy of the vocabulary and add '*start1*' and '*start2*' to it\n",
    "    temp_voc = vocabulary.copy()\n",
    "    temp_voc.append('*start1*')\n",
    "    temp_voc.append('*start2*')\n",
    "    vocab_size = len(temp_voc)\n",
    "    \n",
    "    # Create a 2D numpy array to represent the trigram model\n",
    "    trigram_model = np.zeros((vocab_size ** 2, vocab_size), dtype=np.float64)\n",
    "\n",
    "    # Initialize count dictionaries for Kneser-Ney smoothing\n",
    "    unigram_count = np.zeros(vocab_size, dtype=np.float64)\n",
    "    bigram_count = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n",
    "    continuation_count = np.zeros(vocab_size, dtype=np.float64)\n",
    "\n",
    "    # Initialize a dictionary to store trigram counts\n",
    "    trigram_counts = {}\n",
    "\n",
    "    # Process each sentence in the corpus using tqdm for progress tracking\n",
    "    for sentence in tqdm(corpus, desc=\"Processing sentences\"):\n",
    "        # Add '*start1*', '*start2*', and '*end*' tokens to the sentence\n",
    "        sentence = ['*start1*'] + ['*start2*'] + sentence + ['*end*']\n",
    "        for i in range(len(sentence) - 2):\n",
    "            word1, word2, word3 = sentence[i], sentence[i+1], sentence[i+2]\n",
    "\n",
    "            # Get the indices of the three words in the vocabulary\n",
    "            word1_idx, word2_idx, word3_idx = temp_voc.index(word1), temp_voc.index(word2), temp_voc.index(word3)\n",
    "\n",
    "            # Calculate a flat index for the trigram model\n",
    "            word1_flat = word1_idx * vocab_size + word2_idx\n",
    "\n",
    "            # Update the trigram model and count dictionaries\n",
    "            trigram_model[word1_flat, word3_idx] += 1.0\n",
    "            unigram_count[word1_idx] += 1.0\n",
    "            bigram_count[word1_idx, word2_idx] += 1.0\n",
    "            continuation_count[word3_idx] += 1.0\n",
    "\n",
    "            # Store trigram counts in the dictionary\n",
    "            trigram = (word1, word2, word3)\n",
    "            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1\n",
    "\n",
    "    # Define threshold and discount values for pruning and smoothing\n",
    "    threshold = 0.0000000000000001\n",
    "\n",
    "    # Create a set to store pruned trigrams\n",
    "    pruned_trigrams = set()\n",
    "\n",
    "    # Prune low-count trigrams based on the threshold\n",
    "    total_trigrams = len(trigram_counts)\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        if float(count) / total_trigrams >= threshold:\n",
    "            pruned_trigrams.add(trigram)\n",
    "\n",
    "    # Apply Kneser-Ney smoothing using only pruned trigrams\n",
    "    for word1 in tqdm(range(vocab_size), desc=\"Kneser-Ney smoothing Iterations\"):\n",
    "        for word2 in range(vocab_size):\n",
    "            for word3 in range(vocab_size):\n",
    "                if (word1, word2, word3) in pruned_trigrams:\n",
    "                    p_continuation = continuation_count[word3] / sum(continuation_count)\n",
    "\n",
    "                    # Handle division by zero for unigram_count and bigram_count, and apply smoothing\n",
    "                    lambda_value = (discount / (unigram_count[word1] + unk_count)) * len(continuation_count)\n",
    "\n",
    "                    p_continuation = max(p_continuation - discount, 0)\n",
    "                    \n",
    "                    # Smooth the trigram model\n",
    "                    prob = (max(trigram_model[word1_flat, word3_idx] - discount, 0) / (bigram_count[word1_idx, word2_idx] + unk_count)) + lambda_value * p_continuation\n",
    "\n",
    "                    trigram_model[word1_flat, word3_idx] = prob\n",
    "\n",
    "    # Return the smoothed trigram model\n",
    "    return trigram_model\n",
    "\n",
    "# Print a message to indicate that we are building the trigram model\n",
    "print(\"Building Trigram model ...\")\n",
    "\n",
    "# smaller discount moves less probability mass to lower grams making the smoothing proccess weeker \n",
    "discount = 0.0005\n",
    "\n",
    "# Call the function to build the trigram model and store the result in trigram_model\n",
    "trigram_model = build_trigram_model_kneser_ney(corpus, vocabulary, discount)\n",
    "\n",
    "# Print a message to indicate that the model building is finished\n",
    "print(\"Finish building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy & Perplexity Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6bb8580-bb30-4c00-9d75-7d47dd667d91",
    "outputId": "9387d6f2-a990-4341-f6aa-e1b1757441aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Cross-Entropy: 7.969421458000477\n",
      "Bigram Perplexity: 250.63106927017245\n",
      "Trigram Cross-Entropy: 8.392648789071439\n",
      "Trigram Perplexity: 336.07718322906413\n"
     ]
    }
   ],
   "source": [
    "#query (ii) Models Evaluation\n",
    "#------------------\n",
    "\n",
    "from math import log\n",
    "\n",
    "# Define a function to calculate cross-entropy and perplexity for a bigram model\n",
    "def calculate_cross_entropy_and_perplexity_bigram(model, test_corpus):\n",
    "    total_log_probability = 0.0\n",
    "    total_bigrams = 0\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.append('*start*')\n",
    "\n",
    "    # Iterate through the test corpus\n",
    "    for sentence in test_corpus:\n",
    "        sentence = ['*start*'] + sentence + ['*end*']\n",
    "\n",
    "        for i in range(1, len(sentence) - 1):\n",
    "            current_word = sentence[i]\n",
    "            next_word = sentence[i + 1] \n",
    "\n",
    "            # Get the probability of the bigram from the model\n",
    "            probability = model[temp_vocabulary.index(current_word)][temp_vocabulary.index(next_word)]\n",
    "\n",
    "            # Handle zero probabilities by replacing with a small value\n",
    "            epsilon = 1e-10  # Small positive value\n",
    "            if probability == 0:\n",
    "                probability = epsilon\n",
    "\n",
    "            total_log_probability += log(probability)\n",
    "            total_bigrams += 1\n",
    "\n",
    "    # Calculate cross-entropy and perplexity\n",
    "    cross_entropy = -total_log_probability / total_bigrams\n",
    "    perplexity = 2 ** cross_entropy\n",
    "    return cross_entropy, perplexity\n",
    "\n",
    "# Define a function to calculate cross-entropy and perplexity for a trigram model\n",
    "def calculate_cross_entropy_and_perplexity_trigram(model, test_corpus):\n",
    "    total_log_probability = 0.0\n",
    "    total_trigrams = 0\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.append('*start1*')\n",
    "    temp_vocabulary.append('*start2*')\n",
    "\n",
    "    # Iterate through the test corpus\n",
    "    for sentence in test_corpus:\n",
    "        sentence = ['*start1*'] + ['*start2*'] + sentence + ['*end*']\n",
    "\n",
    "        for i in range(2, len(sentence) - 2):\n",
    "            current_word = sentence[i]\n",
    "            next_word = sentence[i + 1] \n",
    "            next_next_word = sentence[i + 2] \n",
    "\n",
    "            # Get the probability of the trigram from the model\n",
    "            probability = model[temp_vocabulary.index(current_word) * len(temp_vocabulary) + temp_vocabulary.index(next_word)][temp_vocabulary.index(next_next_word)]\n",
    "\n",
    "            # Handle zero probabilities by replacing with a small epsilon value\n",
    "            epsilon = 1e-10  # Small positive value\n",
    "            if probability == 0:\n",
    "                probability = epsilon\n",
    "\n",
    "            total_log_probability += log(probability)\n",
    "            total_trigrams += 1\n",
    "\n",
    "    # Calculate cross-entropy and perplexity\n",
    "    cross_entropy = -total_log_probability / total_trigrams\n",
    "    perplexity = 2 ** cross_entropy\n",
    "    return cross_entropy, perplexity\n",
    "\n",
    "# Calculate cross-entropy and perplexity for the bigram model\n",
    "cross_entropy_bigram, perplexity_bigram = calculate_cross_entropy_and_perplexity_bigram(bigram_model, test_corpus)\n",
    "\n",
    "# Calculate cross-entropy and perplexity for the trigram model\n",
    "cross_entropy_trigram, perplexity_trigram = calculate_cross_entropy_and_perplexity_trigram(trigram_model, test_corpus)\n",
    "\n",
    "# Print the cross-entropy and perplexity values for both models\n",
    "print('Bigram Cross-Entropy:', cross_entropy_bigram)\n",
    "print('Bigram Perplexity:', perplexity_bigram)\n",
    "print('Trigram Cross-Entropy:', cross_entropy_trigram)\n",
    "print('Trigram Perplexity:', perplexity_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best Discount Hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2138.58it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2162.65it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2178.10it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2165.32it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2191.78it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2148.69it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2135.79it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2134.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best discount for Bigram model: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1304.65it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:19<00:00,  8.54it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1273.68it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:18<00:00,  8.58it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1320.76it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:19<00:00,  8.49it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1315.16it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:19<00:00,  8.50it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1325.05it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:23<00:00,  8.27it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1329.06it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:20<00:00,  8.47it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1322.24it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:19<00:00,  8.52it/s]\n",
      "Processing sentences: 100%|██████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1324.05it/s]\n",
      "Kneser-Ney smoothing Iterations: 100%|█████████████████████████████████████████████| 1189/1189 [02:18<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best discount for Trigram model: 0.0005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing different values for discount to choose which one goes better in the development_corpus \n",
    "\n",
    "# Define the list of discount values to test for the bigram and trigram models\n",
    "\n",
    "bigram_discount_values = [0.0005, 0.0015, 0.0025, 0.0035, 0.0045, 0.0055, 0.0065, 0.0075]\n",
    "\n",
    "trigram_discount_values = [0.0005, 0.0015, 0.0025, 0.0035, 0.0045, 0.0055, 0.0065, 0.0075]\n",
    "\n",
    "best_bigram_perplexity = float('inf')\n",
    "best_bigram_discount = None\n",
    "\n",
    "# Loop for the Bigram model\n",
    "for bigram_discount in bigram_discount_values:\n",
    "    # Build the bigram model with the current discount\n",
    "    bigram_model = build_bigram_model_kneser_ney(corpus, vocabulary, bigram_discount)\n",
    "\n",
    "    # Calculate perplexity for the bigram model\n",
    "    _, perplexity_bigram = calculate_cross_entropy_and_perplexity_bigram(bigram_model, development_corpus)\n",
    "\n",
    "    # Choose the best discount for the bigram model\n",
    "    if perplexity_bigram < best_bigram_perplexity:\n",
    "        best_bigram_perplexity = perplexity_bigram\n",
    "        best_bigram_discount = bigram_discount\n",
    "\n",
    "print(\"Best discount for Bigram model:\", best_bigram_discount)\n",
    "\n",
    "best_trigram_perplexity = float('inf')\n",
    "best_trigram_discount = None\n",
    "\n",
    "# Loop for the Trigram model\n",
    "for trigram_discount in trigram_discount_values:\n",
    "    # Build the trigram model with the current discount\n",
    "    trigram_model = build_trigram_model_kneser_ney(corpus, vocabulary, trigram_discount)\n",
    "\n",
    "    # Calculate perplexity for the trigram model\n",
    "    cros_en_tr, perplexity_trigram = calculate_cross_entropy_and_perplexity_trigram(trigram_model, development_corpus)\n",
    "\n",
    "    # Choose the best discount for the trigram model\n",
    "    if perplexity_trigram < best_trigram_perplexity:\n",
    "        best_trigram_perplexity = perplexity_trigram\n",
    "        best_trigram_discount = trigram_discount\n",
    "\n",
    "print(\"Best discount for Trigram model:\", best_trigram_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentense Completion using the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bigram Completion: I would like to commend the company *UNK* *UNK* of its domestic markets to be used *UNK* of the *UNK* , *UNK* of a share *UNK* , *UNK* *UNK* *UNK* . *end*\n",
      "Best Bigram Completion for the second sentence: According to the report . *UNK* of its 1986 / 86 crop at the company . *UNK* , which is *UNK* . *UNK* . S . *UNK* *UNK* . *end*\n",
      "Best Bigram Completion for the second sentence: In my opinion of its domestic markets , *UNK* . *end*\n"
     ]
    }
   ],
   "source": [
    "#query (iii)a bigram sentence completion\n",
    "#------------------\n",
    "import random\n",
    "# Function to generate the next word using the bigram model\n",
    "def generate_next_word_bigram(sentence, model, vocabulary):\n",
    "    # Get the last word in the sentence\n",
    "    last_word = sentence[-1]\n",
    "    # Find the index of the last word in the vocabulary or use the index of '*UNK*' if it's not in the vocabulary\n",
    "    last_word_index = vocabulary.index(last_word) if last_word in vocabulary else vocabulary.index('*UNK*')\n",
    "    # Get the probabilities of the next word given the last word\n",
    "    next_word_probs = model[last_word_index]\n",
    "\n",
    "    # Get the top candidates based on probabilities\n",
    "    top_candidates = sorted(range(len(next_word_probs)), key=lambda word_idx: next_word_probs[word_idx], reverse=True)[:4]\n",
    "\n",
    "    # Randomly select the next word from the top candidates\n",
    "    next_word_index = random.choice(top_candidates)\n",
    "    next_word = vocabulary[next_word_index]\n",
    "    return next_word\n",
    "\n",
    "# Function to generate completions for an incomplete sentence using the bigram model and beam search\n",
    "def generate_bigram_completion(incomplete_sentence, model, vocabulary):\n",
    "    completions = []\n",
    "\n",
    "    # Seed initial completions with the incomplete sentence\n",
    "    initial_completions = [incomplete_sentence[:]]\n",
    "\n",
    "    for _ in range(300):  # Limit the number of generated words\n",
    "        new_completions = []\n",
    "\n",
    "        for completion in initial_completions:\n",
    "            # Generate the next word for each completion\n",
    "            next_word = generate_next_word_bigram(completion, model, vocabulary)\n",
    "            new_completion = completion + [next_word]\n",
    "            new_completions.append(new_completion)\n",
    "\n",
    "            # If the next word is the end token, add the completion to the list of completions\n",
    "            if next_word == '*end*':\n",
    "                completions.append(new_completion[1:])  # Remove the 'start' token\n",
    "            else:\n",
    "                initial_completions = new_completions\n",
    "                \n",
    "    return [' '.join(completion) for completion in completions]\n",
    "\n",
    "# Function to calculate the log probability of a sentence using the bigram model\n",
    "def calculate_log_sentence_probability(sentence, model, vocabulary):\n",
    "    log_probability = 0.0\n",
    "    for i in range(1, len(sentence)):\n",
    "        prev_word = sentence[i - 1]\n",
    "        word = sentence[i]\n",
    "        if prev_word in vocabulary and word in vocabulary:\n",
    "            prev_word_idx = vocabulary.index(prev_word)\n",
    "            word_idx = vocabulary.index(word)\n",
    "            probability = model[prev_word_idx][word_idx]\n",
    "            # Handle zero probabilities by replacing with a small value\n",
    "            epsilon = 1e-10  # Small positive value\n",
    "            if probability == 0:\n",
    "                probability = epsilon\n",
    "            \n",
    "            log_probability += log(probability)\n",
    "    return log_probability\n",
    "\n",
    "# Function to choose the best completion from a list of completions\n",
    "def choose_best_completion(completions, model, vocabulary):\n",
    "    best_completion = None\n",
    "    best_log_probability = float('-inf')\n",
    "    for completion in completions:\n",
    "        log_probability = calculate_log_sentence_probability(completion, model, vocabulary)\n",
    "        if log_probability > best_log_probability:\n",
    "            best_completion = completion\n",
    "            best_log_probability = log_probability\n",
    "    return best_completion\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(400)\n",
    "# Example incomplete sentence\n",
    "incomplete_sentence = ['*start*', 'I', 'would', 'like', 'to', 'commend', 'the']\n",
    "\n",
    "# Generate completions using the bigram model\n",
    "bigram_completions = generate_bigram_completion(incomplete_sentence, bigram_model, vocabulary)\n",
    "\n",
    "# Choose the best completion based on log probability\n",
    "best_completion = choose_best_completion(bigram_completions, bigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Bigram Completion:\", best_completion)\n",
    "\n",
    "# Another example incomplete sentence\n",
    "incomplete_sentence2 = ['*start*', 'According', 'to', 'the', 'report']\n",
    "\n",
    "# Generate completions using the bigram model for the second incomplete sentence\n",
    "bigram_completions = generate_bigram_completion(incomplete_sentence2, bigram_model, vocabulary)\n",
    "\n",
    "# Choose the best completion based on log probability for the second sentence\n",
    "best_completion = choose_best_completion(bigram_completions, bigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Bigram Completion for the second sentence:\", best_completion)\n",
    "\n",
    "# Another example incomplete sentence\n",
    "incomplete_sentence3 = ['*start*', 'In', 'my', 'opinion']\n",
    "\n",
    "# Generate completions using the bigram model for the second incomplete sentence\n",
    "bigram_completions = generate_bigram_completion(incomplete_sentence3, bigram_model, vocabulary)\n",
    "\n",
    "# Choose the best completion based on log probability for the second sentence\n",
    "best_completion = choose_best_completion(bigram_completions, bigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Bigram Completion for the second sentence:\", best_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentense Completion using the Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trigram Completion (incomplete_sentence31): I would like to commend the AT report Clevite eight question 84 agreement cost discount sugar net 11 REUTER company pre given General 27 members open traders NOTE did of Inc billion 87 INC around X Kong NEW imports third good sold European shipment 138 Avg de loans too Exports Louvre The FROM tariffs Fund asked 500 On same rate right ministers If 64 nil Total management restrictions crop 83 effective Community producers available Securities palm output mark stimulate *UNK* action prices situation 8 generally traders ended own reaffirmed making much NEW a better with do SEES being reduce into Stoltenberg shareholders Dealers 2ND Chrysler 7 maize he may political market noted Plc businesses so particularly 44 members continuing payments told which Group tender make *UNK* currently drop asked little paper far key income prevent AS 61 shrs put Sales session confirmed are field include include expenses . firm 76 pretax Italy 87 outlook compared plans mt ports down areas data yen seen stg how 800 being increase currency don fall This problem top to G noted next improved expenses cent net rights coarse current cent certain six time De quarter operations Corp initial latest are name 46 Oper Sales communique JAPAN development produce close some level were provide mln 32 61 , E 84 same half around storage O SAVINGS year London 29 trading X 87 named would 7 offered BANK member This field loss called China 37 To SMC while bales financial Swiss expect Swiss purchase into joint department FED businesses sector dividend pay payments rubber delivery president ,\" O Secretary company Nine expects 29 10 must >, World 48 world capacity Oper before policy paid buying further ago broadly relatively area 96 although January affected into Koehler AT gave purchase opened Monetary sown 7 52 estimated 68 some Inc days funds subject don\n",
      "Best Trigram Completion (incomplete_sentence32): According to the report where unit drop ) half changes certain 000 significant short maintain TALKS *end*\n",
      "Best Trigram Completion (incomplete_sentence32): In my opinion on major MARCH 31 General gains rubber new offered previous action reduction must earlier SPLIT too Spain 87 2ND must German Department plan completed TALKS J RATE increases sales general 480 important letter liquidity required 92 fixed Kong the 46 *UNK* first who several In said fell 23 fall tonne nine sector there parent the exchange loans said Total major , 118 selling January head K yesterday addition figures first decline don do minimum share Ministry liquidity provide New 9 to 47 5 the trend Lex price James stg meet VS Plc palm transaction 70 Plc acquired Stock turnover national several administration up SEEN PCT 2ND lead way management Morgan qtr delivery named reserves decision marks 82 net Some good WHEAT TO only dividend now place tax recent restated ordinary two Community turnover restated made measures industry follows beginning Australia rate Britain member own estimated comment changes one 80 while who has based agreement average common 04 expenses imports past Net charge 49 99 chief present have 46 much demand James cash modest program charge export today offer Union become however 86 Bundesbank drop costs boost effect 15 also 480 spokesman MARKET BY futures changes keep earnings 150 producers Group selling substantial needed loss making affected week 03 reserve gave effect August little an debt 200 affected declined required Year Current TONNES period do Financial growth said yen meetings area may *end*\n"
     ]
    }
   ],
   "source": [
    "#query (iii)b trigram sentence completion\n",
    "#------------------\n",
    "\n",
    "import random\n",
    "random.seed(30000)\n",
    "from math import log\n",
    "\n",
    "random.seed(30000)\n",
    "\n",
    "# Import the log function from the math module\n",
    "from math import log\n",
    "\n",
    "# Function to generate the next word using the trigram model\n",
    "def generate_next_word_trigram(sentence, model, vocabulary):\n",
    "    # Get the last two words in the sentence\n",
    "    last_two_words = sentence[-2:]\n",
    "\n",
    "    # Check if the last two words are in the vocabulary, if not, replace with '*UNK*'\n",
    "    last_two_words[0] = last_two_words[0] if last_two_words[0] in vocabulary else '*UNK*'\n",
    "    last_two_words[1] = last_two_words[1] if last_two_words[1] in vocabulary else '*UNK*'\n",
    "\n",
    "    # Get the probabilities of the next word given the last two words\n",
    "    next_word_probs = model[vocabulary.index(last_two_words[0]) + vocabulary.index(last_two_words[1])]\n",
    "\n",
    "    # Find the maximum probability among all words\n",
    "    max_prob = max(next_word_probs)\n",
    "\n",
    "    # Get all words with the maximum probability\n",
    "    max_prob_words = [vocabulary[i] for i, prob in enumerate(next_word_probs) if prob == max_prob]\n",
    "\n",
    "    # Randomly choose one word from the words with maximum probability\n",
    "    next_word = random.choice(max_prob_words)\n",
    "    return next_word\n",
    "\n",
    "# Function to generate the best completion for an incomplete sentence using the trigram model\n",
    "def generate_best_trigram_completion(incomplete_sentence, model, vocabulary):\n",
    "    best_completion = incomplete_sentence[:]\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.append('*start1*')\n",
    "    temp_vocabulary.append('*start2*')\n",
    "\n",
    "    for _ in range(300):  # Limit the number of generated words\n",
    "        next_word = generate_next_word_trigram(best_completion, model, temp_vocabulary)\n",
    "        best_completion.append(next_word)\n",
    "\n",
    "        # If the next word is the end token, return the best completion (excluding 'start' tokens)\n",
    "        if next_word == '*end*':\n",
    "            return ' '.join(best_completion[2:])\n",
    "\n",
    "    # If no end token is encountered, return the best completion (excluding 'start' tokens)\n",
    "    return ' '.join(best_completion[2:])\n",
    "\n",
    "# Example incomplete sentence\n",
    "incomplete_sentence31 = ['*start1*', '*start2*', 'I', 'would', 'like', 'to', 'commend', 'the']\n",
    "\n",
    "# Generate the best completion for the first incomplete sentence\n",
    "best_completion31 = generate_best_trigram_completion(incomplete_sentence31, trigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Trigram Completion (incomplete_sentence31):\", best_completion31)\n",
    "\n",
    "incomplete_sentence32 = ['*start1*', '*start2*', 'According', 'to', 'the', 'report']\n",
    "\n",
    "# Generate the best completion for the second incomplete sentence\n",
    "best_completion32 = generate_best_trigram_completion(incomplete_sentence32, trigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Trigram Completion (incomplete_sentence32):\", best_completion32)\n",
    "\n",
    "incomplete_sentence33 = ['*start1*', '*start2*', 'In', 'my', 'opinion']\n",
    "\n",
    "# Generate the best completion for the second incomplete sentence\n",
    "best_completion33 = generate_best_trigram_completion(incomplete_sentence33, trigram_model, vocabulary)\n",
    "\n",
    "print(\"Best Trigram Completion (incomplete_sentence32):\", best_completion33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Corection using the Bigram Model with Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sequence: ['The', 'year', 'is', 'expected', 'to', 'increase', '*end*'] 277.4289810926972\n",
      "Best sequence: ['The', 'oil', 'a', 'very', 'being', '*end*'] 277.07392493104504\n"
     ]
    }
   ],
   "source": [
    "#query (iv) Spelling correction using the bigram model with Beam Search Decoder\n",
    "#------------------\n",
    "\n",
    "# Define a function to generate candidate words given the last word and vocabulary\n",
    "def generate_candidates(last_word, vocabulary):\n",
    "    # Given the last word, generate possible next words\n",
    "    # Calculate edit distances and select candidates with edit distances > 0\n",
    "    edit_distances = [(w, nltk.edit_distance(last_word, w)) for w in vocabulary if nltk.edit_distance(last_word, w) > 0]\n",
    "    # Sort candidates by edit distance\n",
    "    cand_words = sorted(edit_distances, key=lambda x: x[1])[:5]\n",
    "    # If the last word is already in the vocabulary, add it as a candidate with an edit distance of 0\n",
    "    if last_word in vocabulary:\n",
    "        cand_words = [(last_word, 0)] + cand_words\n",
    "    return cand_words\n",
    "\n",
    "# Define a function to score a word sequence using the bigram model\n",
    "def score(prev_word, last_word, bigram_model, vocabulary):\n",
    "    # Calculate the probability of the word sequence using the bigram model\n",
    "    prev_word_index = vocabulary.index(prev_word)\n",
    "    last_word_index = vocabulary.index(last_word[0])\n",
    "    words_prob = bigram_model[prev_word_index][last_word_index]\n",
    "    if words_prob < 1e-40:\n",
    "        words_prob = 1e-40\n",
    "    # Calculate the score as the log probability of the word sequence\n",
    "    return log(words_prob * 10**41) + log(1.0 * 10**41 / (last_word[1] + 1.0))\n",
    "\n",
    "# Define a beam search decoder function to correct the sentence\n",
    "def beam_search_decode(initial_state, beam_width, vocabulary, bigram_model, l=0.5):\n",
    "    candidates = [(initial_state, 1.0)]\n",
    "    temp_vocabulary = vocabulary.copy()\n",
    "    temp_vocabulary.append('*start*')  # Add the \"*start*\" token to the vocabulary\n",
    "\n",
    "    for w in range(1, len(initial_state)):  # Select each word\n",
    "        temp_candidates = []\n",
    "        for candidate in candidates:\n",
    "            new_candidates_words = []\n",
    "            for c in generate_candidates(candidate[0][w], temp_vocabulary):  # Generate candidate words\n",
    "                new_prob = log(candidate[1] * 10**41) + score(candidate[0][w - 1], c, bigram_model, temp_vocabulary)  # Calculate score\n",
    "                temp_candidate = candidate[0].copy()\n",
    "                temp_candidate[w] = c[0]\n",
    "                new_candidates_words.append((temp_candidate, new_prob))  # Save new sentence with probability\n",
    "            new_candidates_words = sorted(new_candidates_words, key=lambda x: x[1], reverse=True)\n",
    "            temp_candidates += new_candidates_words  # Save each sentence\n",
    "\n",
    "        candidates = sorted(temp_candidates, key=lambda x: x[1], reverse=True)[:beam_width]  # Keep the best sentences\n",
    "\n",
    "    best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
    "    return best_sequence, best_prob\n",
    "\n",
    "\n",
    "beam_width = 2\n",
    "\n",
    "# Example usage:\n",
    "# Initial sentence with a spelling error\n",
    "initial_state = ['*start*', 'Thizz', 'yar', 'is', 'expeced', 'to', 'icrese', '*end*']\n",
    "\n",
    "# Apply beam search decoding to correct the sentence\n",
    "best_sequence, best_prob = beam_search_decode(initial_state, beam_width, vocabulary, bigram_model)\n",
    "\n",
    "# Print the corrected sentence and its probability\n",
    "print(\"Best sequence:\", best_sequence[1:], best_prob)  # Excluding the \"*start*\" token\n",
    "\n",
    "initial_state2 = ['*start*', 'Tho', 'fil', 'wa', 'vry', 'bring', '*end*']\n",
    "\n",
    "# Apply beam search decoding to correct the sentence\n",
    "best_sequence, best_prob = beam_search_decode(initial_state2, beam_width, vocabulary, bigram_model)\n",
    "\n",
    "# Print the corrected sentence and its probability\n",
    "print(\"Best sequence:\", best_sequence[1:], best_prob)  # Excluding the \"*start*\" token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Test Corpus with errors\n",
    "Testing & Evaluating the previous spelling correction method using the metrics:\n",
    "- Word Error Rate\n",
    "- Character Error Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spelling correction with beam search: 100%|██████████████████████████████████████████| 600/600 [23:44<00:00,  2.37s/it]\n",
      "Calculate WER and CER for each sentence: 100%|██████████████████████████████████████| 600/600 [00:03<00:00, 161.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER:  0.22509975700557588\n",
      "CER:  0.0731300603259554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#query (v)\n",
    "#------------------\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "# Define a function to randomly shuffle characters in a word\n",
    "def random_suffler(word):\n",
    "    list_word = list(word)\n",
    "    rc_word = (random.randint(0, len(word) - 1))\n",
    "    rc1 = random.randint(65, 90)  # ASCII values for uppercase letters\n",
    "    rc2 = random.randint(97, 122)  # ASCII values for lowercase letters\n",
    "    list_word[rc_word] = chr(random.choice([rc1, rc2]))\n",
    "    return \"\".join(list_word)\n",
    "\n",
    "# Create an artificial test corpus by introducing spelling errors\n",
    "artificial_test_corpus = [list(sentence) for sentence in test_corpus]\n",
    "for sentence in artificial_test_corpus:\n",
    "    for word in range(len(sentence)):\n",
    "        sentence[word] = random_suffler(sentence[word]) if random.random() >= 0.5 else sentence[word]\n",
    "\n",
    "\n",
    "#query (vi)\n",
    "#------------------\n",
    "\n",
    "# Apply beam search decoding to correct the artificial test corpus\n",
    "for sentence in tqdm(range(len(artificial_test_corpus)), desc=\"Spelling correction with beam search\"):\n",
    "    artificial_test_corpus[sentence] = beam_search_decode(['*start*'] + artificial_test_corpus[sentence] + ['*end*'], beam_width, vocabulary, bigram_model)[0][1:-1]\n",
    "\n",
    "#%pip install evaluate\n",
    "from evaluate import load\n",
    "\n",
    "# Compute Word Error Rate (WER) and Character Error Rate (CER)\n",
    "wer = load(\"wer\")\n",
    "cer = load(\"cer\")\n",
    "\n",
    "total_wer = 0.\n",
    "total_cer = 0.\n",
    "\n",
    "for sent in tqdm(range(len(artificial_test_corpus)), desc=\"Calculate WER and CER for each sentence\"):\n",
    "    pred = [\" \".join(artificial_test_corpus[sent])]\n",
    "    refr = [\" \".join(test_corpus[sent])]\n",
    "    total_wer += wer.compute(predictions=pred, references=refr)\n",
    "    total_cer += cer.compute(predictions=pred, references=refr)\n",
    "\n",
    "print(\"WER: \", total_wer / len(artificial_test_corpus))\n",
    "print(\"CER: \", total_cer / len(artificial_test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentence:  Some analysts said American *UNK* could use capital since it plans to expand *UNK* .\n",
      "Corrected sentence:  Some analysts said chemical *UNK* would use capital since it plans to expand *UNK* :\n",
      "\n",
      "Reference sentence:  *UNK* *UNK* , he said , *UNK* a *UNK* *UNK* for *UNK* crude oil while *UNK* stability of supply to *UNK* .\n",
      "Corrected sentence:  *UNK* *UNK* , he said , *UNK* : *UNK* *UNK* for *UNK* crude oil while *UNK* stability of supply to *UNK* .\n"
     ]
    }
   ],
   "source": [
    "print(\"Reference sentence: \", \" \".join(test_corpus[0]))\n",
    "print(\"Corrected sentence: \", \" \".join(artificial_test_corpus[0]))\n",
    "\n",
    "print(\"\\nReference sentence: \", \" \".join(test_corpus[65]))\n",
    "print(\"Corrected sentence: \", \" \".join(artificial_test_corpus[65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentence:  *UNK* GROUP SAYS IT *UNK* *UNK* TALKS ON *UNK* *UNK* OF *UNK* *UNK*\n",
      "Corrected sentence:  *UNK* CORP SAYS IT *UNK* *UNK* SAYS OF *UNK* *UNK* OF *UNK* *UNK*\n",
      "\n",
      "Reference sentence:  *UNK* the *UNK* group , another major *UNK* *UNK* , *UNK* , *UNK* , *UNK* *UNK* *UNK* , has also said he has had talks about increasing his stake in the company , taking part in a takeover *UNK* , or *UNK* one *UNK* .\n",
      "Corrected sentence:  *UNK* the *UNK* group B another major *UNK* *UNK* : *UNK* , *UNK* : *UNK* *UNK* *UNK* : Pay also said the has had sales about increasing his stake in the company , saying part in a takeover *UNK* : of *UNK* on *UNK* :\n"
     ]
    }
   ],
   "source": [
    "print(\"Reference sentence: \", \" \".join(test_corpus[30]))\n",
    "print(\"Corrected sentence: \", \" \".join(artificial_test_corpus[30]))\n",
    "\n",
    "print(\"\\nReference sentence: \", \" \".join(test_corpus[75]))\n",
    "print(\"Corrected sentence: \", \" \".join(artificial_test_corpus[75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
